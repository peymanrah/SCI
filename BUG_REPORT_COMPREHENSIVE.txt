================================================================================
                    COMPREHENSIVE BUG REPORT - SCI CODEBASE
                         Line-by-Line Code Review
              Updated: December 7, 2025 (Second Review Pass)
================================================================================

This report contains:
- PART 1: VERIFICATION STATUS OF PREVIOUS 35 BUGS  
- PART 2: NEW BUGS FOUND IN SECOND LINE-BY-LINE REVIEW

================================================================================
                      PART 1: PREVIOUS BUG VERIFICATION
                      (35 bugs from first review)
================================================================================

------------------------------------------------------------------------------
                          CRITICAL SEVERITY
------------------------------------------------------------------------------

CRITICAL #1: train.py uses dictionary access but load_config returns dataclass
STATUS: ✅ FIXED
Evidence: train.py now uses config.training.batch_size, config.loss.scl_weight, 
          config.data.split (dataclass attribute access throughout)

CRITICAL #2: train.py uses wrong config paths  
STATUS: ✅ FIXED
Evidence: train.py uses config.data.split (not scan_split), derives 
          experiment_name from Path(args.config).stem

CRITICAL #3: train.py validate() calls evaluator.evaluate() with DataLoader
STATUS: ⚠️ STILL PRESENT - NEEDS FIX
File: train.py line 264
Code: eval_results = evaluator.evaluate(model, val_loader, device=device)
Issue: SCIEvaluator.evaluate() takes config in __init__ and dataset_configs
       in evaluate(), not a dataloader.

CRITICAL #4: run_ablations.py uses wrong training script arguments
STATUS: ✅ FIXED
Evidence: train.py now accepts --wandb-project and --wandb-run-name (lines 65-70)

CRITICAL #5: train.py passes nonexistent pair_generator to SCANDataCollator
STATUS: ✅ FIXED
Evidence: SCANDataCollator now accepts pair_generator parameter (line 48)

CRITICAL #6: train.py calls SCANDataset with wrong constructor arguments
STATUS: ✅ FIXED
Evidence: SCANDataset now takes tokenizer as first positional and split_name

------------------------------------------------------------------------------
                           HIGH SEVERITY
------------------------------------------------------------------------------

HIGH #7: CheckpointingConfig.keep_last_n is never used
STATUS: ✅ FIXED
Evidence: CheckpointManager now uses save_total_limit with fallback to keep_last_n
          (train.py line 418: getattr(config.checkpointing, 'save_total_limit', 
           getattr(config.checkpointing, 'keep_last_n', 3)))

HIGH #8: LoggingConfig fields are defined but never read
STATUS: ⚠️ STILL PRESENT - LOW PRIORITY
Issue: log_dir, results_dir, checkpoint_every_n_epochs still unused

HIGH #9: Evaluator per-example processing is O(n^2) slow
STATUS: ⚠️ STILL PRESENT - NEEDS FIX
File: sci/evaluation/evaluator.py lines 150-220
Issue: Still iterates example-by-example with single-item generation

HIGH #10: SCANDataset has deprecated SCANCollator class with wrong assumptions
STATUS: ⚠️ STILL PRESENT - LOW PRIORITY
File: sci/data/datasets/scan_dataset.py lines 217-258
Issue: Deprecated class still references input_ids but __getitem__ returns strings

HIGH #11: content_encoder.py doesn't re-apply instruction_mask after each layer
STATUS: ✅ FIXED
Evidence: content_encoder.py line 202-206 now reapplies mask after transformer layers:
          "# HIGH #11 FIX: Reapply instruction mask AFTER transformer layers"

HIGH #12: causal_binding.py position queries use fixed max_seq_len
STATUS: ⚠️ PARTIALLY FIXED - NEEDS REVIEW
File: sci/models/components/causal_binding.py line 103
Issue: Still uses base_max_seq_len=1024, dynamic extension logic needs verification

HIGH #13: combined_loss.py EOS loss computed on wrong dimension
STATUS: ✅ FIXED
Evidence: EOS loss now uses label positions (combined_loss.py lines 250-280)
          finds positions where labels == eos_token_id

HIGH #14: scan_pair_generator.py doesn't handle empty batches
STATUS: ✅ FIXED
Evidence: get_batch_pair_labels() line 209 now checks:
          if len(batch_items) == 0: return torch.zeros((0, 0), dtype=torch.long)

HIGH #15: scan_extractor.py CONTENT_WORDS misses 'turn'
STATUS: ✅ FIXED
Evidence: CONTENT_WORDS now includes 'turn' and 'TURN' (line 42-43)

------------------------------------------------------------------------------
                          MEDIUM SEVERITY
------------------------------------------------------------------------------

MEDIUM #16: trainer.py doesn't use gradient_accumulation_steps from config
STATUS: ✅ FIXED
Evidence: train.py lines 157-158 now implements gradient accumulation:
          grad_accum_steps = getattr(config.training, 'gradient_accumulation_steps', 1)

MEDIUM #17: trainer.py log_every uses hardcoded 10
STATUS: ✅ FIXED
Evidence: train.py line 154: log_every = getattr(config.logging, 'log_every', 10)

MEDIUM #18: slot_attention.py epsilon in softmax denominator not configurable
STATUS: ⚠️ STILL PRESENT - LOW PRIORITY
File: sci/models/components/slot_attention.py
Issue: eps=1e-8 still hardcoded

MEDIUM #19: positional_encoding.py rotary base hardcoded
STATUS: ⚠️ NEEDS VERIFICATION
Issue: Need to verify base frequency matches TinyLlama

MEDIUM #20: abstraction_layer.py structural_weight clamping affects gradients
STATUS: ⚠️ STILL PRESENT - LOW PRIORITY
Issue: Redundant clamp after sigmoid still present

MEDIUM #21: data_leakage_checker.py doesn't check for near-duplicates
STATUS: ⚠️ STILL PRESENT - LOW PRIORITY
Issue: Only checks exact match, not semantic equivalence

MEDIUM #22: config_loader.py doesn't validate injection_layers
STATUS: ✅ FIXED
Evidence: config_loader.py lines 410-415 now validates:
          "assert layer < num_decoder_layers"

MEDIUM #23: scl_loss.py temperature not applied consistently
STATUS: ⚠️ NEEDS VERIFICATION
Issue: Need to verify temperature application in all paths

MEDIUM #24: train.py has circular import risk
STATUS: ⚠️ STILL PRESENT - LOW PRIORITY
Issue: Top-level imports still exist

MEDIUM #25: scripts/train_sci.py duplicates logic from train.py
STATUS: ⚠️ STILL PRESENT - LOW PRIORITY
Issue: Two training scripts still exist with different logic

------------------------------------------------------------------------------
                           LOW SEVERITY
------------------------------------------------------------------------------

LOW #26-35: Code quality issues
STATUS: Mixed - Most still present but low priority
- Missing type hints (still present)
- Inconsistent logging (still present)  
- [OK] prefixes inconsistent (still present)
- Unused imports (still present)
- Magic numbers (still present)

================================================================================
                      PART 2: NEW BUGS FOUND
                      (Second Line-by-Line Review)
================================================================================

------------------------------------------------------------------------------
                      NEW CRITICAL BUGS
------------------------------------------------------------------------------

NEW CRITICAL #36: train.py calls non-existent is_overfitting() method
--------------------------------------------------------------------------------
File: train.py line 509
Code:
    is_overfitting, loss_ratio = overfitting_detector.is_overfitting()
    if is_overfitting:
        
Issue: OverfittingDetector class (sci/training/early_stopping.py) does NOT have
       an is_overfitting() method. It only has update() which returns the result.
       
Impact: Training will crash with AttributeError at end of each epoch.

Fix: Change to:
    is_overfitting, loss_ratio = overfitting_detector.update(
        metrics['train_loss'], metrics['val_loss'], epoch
    )


NEW CRITICAL #37: train.py calls validate() which calls wrong evaluator API
--------------------------------------------------------------------------------
File: train.py lines 264, 473
Code: eval_results = evaluator.evaluate(model, val_loader, device=device)

Issue: SCIEvaluator.evaluate() signature is:
       evaluate(self, model, dataset_configs=None) -> Dict[str, Dict[str, float]]
       
       It takes dataset_configs (list of dicts), not a dataloader.
       
Impact: Validation will crash with TypeError every epoch.

Fix: Need to refactor validate() to either:
     1. Create a SCANEvaluator and call with proper dataset_configs, or
     2. Use evaluator._evaluate_dataset() directly with dataset


NEW CRITICAL #38: SCANEvaluator.__init__ requires config but SCANEvaluator usage varies
--------------------------------------------------------------------------------
File: sci/evaluation/evaluator.py line 30
Code: def __init__(self, config):

File: train.py line 410  
Code: evaluator = SCANEvaluator(tokenizer, eval_config=eval_config)

Issue: SCIEvaluator.__init__ takes config as first parameter (the full config),
       but train.py passes tokenizer as first parameter.
       
Impact: TypeError - wrong argument type passed to evaluator constructor.


------------------------------------------------------------------------------
                        NEW HIGH BUGS
------------------------------------------------------------------------------

NEW HIGH #39: OverfittingDetector.update() called but result ignored in trainer.py
--------------------------------------------------------------------------------
File: sci/training/trainer.py lines 400-420 (train_epoch)

Issue: In the class-based SCITrainer, overfitting detection is completely
       missing from the training loop.
       
Impact: Overfitting won't be detected when using SCITrainer class.


NEW HIGH #40: train.py uses config.training.epochs but config has max_epochs
--------------------------------------------------------------------------------
File: train.py lines 393, 456, 460

Issue: Uses config.training.epochs but TrainingConfig dataclass has max_epochs
       (line 164 of config_loader.py). There's also an "epochs" alias but
       inconsistent usage could cause issues.
       
Impact: May use wrong value if only one is set in YAML.


NEW HIGH #41: SCANDataCollator expected to handle 'commands' key but gets 'command'
--------------------------------------------------------------------------------
File: sci/data/scan_data_collator.py line 116
Code: commands = [f['commands'] for f in features]

File: sci/data/datasets/scan_dataset.py line 206  
Code: return {"commands": command, ...}

STATUS: Actually OK - both use "commands" (plural). Verified correct.


NEW HIGH #42: train.py validation eval_results keys may not match expected
--------------------------------------------------------------------------------
File: train.py lines 476-478
Code:
    'val_exact_match': eval_results['exact_match'],
    'val_token_accuracy': eval_results['token_accuracy'],
    
Issue: eval_results from SCIEvaluator.evaluate() returns nested dict:
       {dataset_name: {'exact_match': ..., 'token_accuracy': ...}}
       
       But train.py expects flat dict with keys 'exact_match' and 'token_accuracy'
       
Impact: KeyError when accessing evaluation results.


------------------------------------------------------------------------------
                       NEW MEDIUM BUGS
------------------------------------------------------------------------------

NEW MEDIUM #43: Trainer mixed precision conditional has wrong scaler check
--------------------------------------------------------------------------------
File: sci/training/trainer.py lines 373-385
Code: with autocast() if self.scaler else torch.enable_grad():

Issue: autocast() should be used when mixed_precision is enabled, not based on
       scaler existence. Also, torch.enable_grad() is a no-op in training.
       
Impact: May incorrectly skip autocast in some configurations.


NEW MEDIUM #44: CheckpointManager expects config but receives dataclass  
--------------------------------------------------------------------------------
File: sci/training/checkpoint_manager.py line 48
Code: 'config': config if isinstance(config, dict) else vars(config),

Issue: vars() doesn't work properly on dataclasses. Should use asdict().

Impact: Checkpoint config may not serialize properly.


NEW MEDIUM #45: train.py early stopping uses val_exact_match but evaluator broken
--------------------------------------------------------------------------------
File: train.py line 492
Code: should_stop = early_stopping(metrics['val_exact_match'], epoch)

Issue: Because evaluator is broken (BUG #37-38), val_exact_match will never be
       computed correctly, so early stopping based on accuracy won't work.
       
Impact: Training may run for all epochs without early stopping.


NEW MEDIUM #46: Trainer warmup_steps may exceed total_steps
--------------------------------------------------------------------------------
File: sci/training/trainer.py line 101
Code:
    total_steps = len(self.train_loader) * config.training.max_epochs
    self.scheduler = get_linear_schedule_with_warmup(
        self.optimizer,
        num_warmup_steps=config.training.warmup_steps,
        
Issue: If warmup_steps in config exceeds total_steps, scheduler behaves unexpectedly.
       No validation that warmup_steps < total_steps.
       
Impact: Learning rate schedule may be incorrect.


NEW MEDIUM #47: scan_dataset.py test file section uses deprecated API
--------------------------------------------------------------------------------
File: sci/data/datasets/scan_dataset.py lines 285-340

Issue: The __main__ test section creates SCANCollator and expects input_ids
       from __getitem__, but __getitem__ returns raw strings. The test will fail.
       
Impact: Self-test code doesn't work; confusing for developers.


------------------------------------------------------------------------------
                        NEW LOW BUGS
------------------------------------------------------------------------------

NEW LOW #48: Duplicate parameter counting in trainer fairness metrics
--------------------------------------------------------------------------------
File: sci/training/trainer.py lines 281-295

Issue: Parameters are counted multiple times in nested loops.
       Could be more efficient with single pass.


NEW LOW #49: wandb.watch called without model being fully initialized
--------------------------------------------------------------------------------
File: sci/training/trainer.py line 122
Code: wandb.watch(self.model, log='all', log_freq=100)

Issue: Called in __init__ which may be before model is fully on device.


NEW LOW #50: Multiple config access patterns in same file
--------------------------------------------------------------------------------
Files: train.py, trainer.py

Issue: Some code uses config.x.y, some uses getattr(config.x, 'y', default).
       Inconsistent patterns make code harder to maintain.


================================================================================
                              SUMMARY
================================================================================

PREVIOUS BUGS STATUS (35 total):
| Status      | Count |
|-------------|-------|
| ✅ FIXED    |  16   |
| ⚠️ STILL    |  11   |
| Low Priority|   8   |

NEW BUGS FOUND (15 total):
| Severity | Count |
|----------|-------|
| CRITICAL |   3   |
| HIGH     |   4   |  
| MEDIUM   |   5   |
| LOW      |   3   |


================================================================================
                         PRIORITY FIX ORDER
================================================================================

IMMEDIATE (Will crash training):
1. NEW CRITICAL #36: train.py calls non-existent is_overfitting() method
2. NEW CRITICAL #37: train.py validate() calls wrong evaluator API  
3. NEW CRITICAL #38: SCANEvaluator constructor mismatch
4. CRITICAL #3 (still present): evaluator.evaluate() API mismatch

HIGH PRIORITY (Incorrect behavior):
5. NEW HIGH #40: config.training.epochs vs max_epochs inconsistency
6. NEW HIGH #42: eval_results nested dict vs flat dict mismatch
7. HIGH #9 (still present): Evaluator O(n²) slowness

MEDIUM PRIORITY:
8. NEW MEDIUM #44: CheckpointManager config serialization
9. NEW MEDIUM #45: Early stopping broken due to evaluator issues
10. NEW MEDIUM #46: warmup_steps validation

LOW PRIORITY:
- All remaining bugs marked as low priority or "still present"


================================================================================
                         RECOMMENDED FIXES
================================================================================

1. train.py line 509 - Change:
   FROM: is_overfitting, loss_ratio = overfitting_detector.is_overfitting()
   TO:   is_overfitting, loss_ratio = overfitting_detector.update(
             metrics['train_loss'], metrics['val_loss'], epoch
         )

2. train.py validate() function - Refactor to use proper evaluator API:
   - Create proper dataset_configs list
   - Call evaluator.evaluate() with correct parameters
   - Or create separate simple validation function

3. train.py line 410 - Fix SCANEvaluator instantiation:
   FROM: evaluator = SCANEvaluator(tokenizer, eval_config=eval_config)
   TO:   evaluator = SCANEvaluator(config)

4. train.py lines 476-478 - Handle nested eval_results:
   dataset_key = f"{config.data.split}_test"
   if dataset_key in eval_results:
       metrics['val_exact_match'] = eval_results[dataset_key]['exact_match']


================================================================================
                           END OF REPORT
================================================================================
