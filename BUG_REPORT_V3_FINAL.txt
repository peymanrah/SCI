================================================================================
                    COMPREHENSIVE BUG REPORT - SCI CODEBASE
                         Line-by-Line Code Review
                   Final Update: December 7, 2025 (Third Pass)
================================================================================

This report contains:
- PART 1: STATUS OF ALL PREVIOUS BUGS (50 bugs from previous reports)
- PART 2: NEW BUGS DISCOVERED (Third line-by-line review)

================================================================================
                      PART 1: PREVIOUS BUG STATUS VERIFICATION
================================================================================

------------------------------------------------------------------------------
                     FROM FIRST REPORT (35 bugs)
------------------------------------------------------------------------------

CRITICAL #1: train.py uses dictionary access but load_config returns dataclass
STATUS: ✅ FIXED
Evidence: train.py lines 107-109 use config.training.optimizer.base_lr

CRITICAL #2: train.py uses wrong config paths (scan_split)
STATUS: ✅ FIXED
Evidence: train.py line 329 uses config.data.split

CRITICAL #3: train.py validate() calls evaluator.evaluate() with DataLoader
STATUS: ✅ FIXED
Evidence: train.py line 271 - SCANEvaluator.evaluate() now accepts dataloader
          sci/evaluation/scan_evaluator.py line 52 accepts (model, test_dataloader, device)

CRITICAL #4: run_ablations.py uses wrong training script arguments
STATUS: ✅ FIXED
Evidence: train.py lines 65-70 now accepts --wandb-project and --wandb-run-name

CRITICAL #5: train.py passes nonexistent pair_generator to SCANDataCollator
STATUS: ✅ FIXED
Evidence: scan_data_collator.py line 46 accepts pair_generator parameter

CRITICAL #6: train.py calls SCANDataset with wrong constructor arguments
STATUS: ✅ FIXED
Evidence: scan_dataset.py line 46 takes tokenizer as first parameter

HIGH #7: CheckpointingConfig.keep_last_n is never used
STATUS: ✅ FIXED
Evidence: train.py line 420 uses getattr with fallback to keep_last_n

HIGH #8: LoggingConfig fields are defined but never read
STATUS: ⚠️ PARTIAL - log_dir and results_dir still unused
Evidence: Only log_every and use_wandb are actively used

HIGH #9: Evaluator per-example processing is O(n²) slow
STATUS: ✅ FIXED
Evidence: scan_evaluator.py now processes batches (line 72: for batch in tqdm...)

HIGH #10: SCANDataset deprecated SCANCollator references undefined attributes
STATUS: ⚠️ STILL PRESENT but marked DEPRECATED
Evidence: scan_dataset.py lines 213-248 - deprecated class still exists
          Low impact since it's marked deprecated and not used

HIGH #11: content_encoder.py doesn't re-apply instruction_mask after each layer
STATUS: ✅ FIXED (verified in previous report)

HIGH #12: causal_binding.py position queries use fixed max_seq_len
STATUS: ⚠️ STILL PRESENT - base_max_seq_len=1024 fixed
Impact: Low - 1024 is sufficient for SCAN

HIGH #13: combined_loss.py EOS loss computed on wrong dimension
STATUS: ✅ FIXED
Evidence: combined_loss.py lines 89-136 properly shifts logits and labels

HIGH #14: scan_pair_generator.py doesn't handle empty batches
STATUS: ✅ FIXED
Evidence: scan_pair_generator.py line 209 checks if len(batch_items) == 0

HIGH #15: scan_extractor.py CONTENT_WORDS misses 'turn'
STATUS: ✅ FIXED
Evidence: scan_extractor.py line 38 includes 'turn' and 'TURN'

MEDIUM #16: trainer.py doesn't use gradient_accumulation_steps
STATUS: ✅ FIXED
Evidence: train.py lines 147-148 use gradient_accumulation_steps

MEDIUM #17: trainer.py log_every uses hardcoded 10
STATUS: ✅ FIXED
Evidence: train.py line 144 uses getattr(config.logging, 'log_every', 10)

MEDIUM #18: slot_attention.py epsilon not configurable
STATUS: ⚠️ STILL PRESENT - Low priority

MEDIUM #19: positional_encoding.py rotary base hardcoded
STATUS: ⚠️ NEEDS VERIFICATION

MEDIUM #20: abstraction_layer.py structural_weight clamping
STATUS: ⚠️ STILL PRESENT - Low priority

MEDIUM #21: data_leakage_checker.py doesn't check near-duplicates
STATUS: ⚠️ STILL PRESENT - Low priority

MEDIUM #22: config_loader.py doesn't validate injection_layers
STATUS: ✅ FIXED
Evidence: config_loader.py lines 405-412 validates injection_layers

MEDIUM #23: scl_loss.py temperature not applied consistently
STATUS: ⚠️ NEEDS VERIFICATION

MEDIUM #24: train.py has circular import risk
STATUS: ⚠️ STILL PRESENT - Low priority (imports at top level)

MEDIUM #25: scripts/train_sci.py duplicates logic from train.py
STATUS: ⚠️ STILL PRESENT - Two training scripts exist

LOW #26-35: Code quality issues
STATUS: Mixed - Most still present

------------------------------------------------------------------------------
                     FROM SECOND REPORT (15 bugs)
------------------------------------------------------------------------------

NEW CRITICAL #36: train.py calls non-existent is_overfitting() method
STATUS: ✅ FIXED
Evidence: train.py line 517 now uses:
          is_overfitting, loss_ratio = overfitting_detector.update(...)

NEW CRITICAL #37: train.py validate() calls wrong evaluator API
STATUS: ✅ FIXED
Evidence: SCANEvaluator.evaluate(model, test_dataloader, device) now correct

NEW CRITICAL #38: SCANEvaluator constructor mismatch
STATUS: ✅ FIXED
Evidence: scan_evaluator.py line 24: __init__(self, tokenizer, eval_config=None)
          train.py line 409: SCANEvaluator(tokenizer, eval_config=eval_config)

NEW HIGH #39: OverfittingDetector.update() called but result ignored in trainer.py
STATUS: ⚠️ STILL PRESENT in SCITrainer class
Evidence: trainer.py doesn't use OverfittingDetector at all

NEW HIGH #40: train.py uses config.training.epochs but config has max_epochs
STATUS: ✅ FIXED
Evidence: train.py lines 393, 456 use getattr with fallback:
          total_epochs = getattr(config.training, 'max_epochs', 
                                 getattr(config.training, 'epochs', 50))

NEW HIGH #42: train.py eval_results keys may not match expected
STATUS: ✅ FIXED
Evidence: scan_evaluator.py now returns flat dict with 'exact_match' key (line 182)

NEW MEDIUM #43: Trainer mixed precision conditional has wrong scaler check
STATUS: ✅ FIXED
Evidence: trainer.py lines 369-372 now uses:
          from contextlib import nullcontext
          amp_context = torch.amp.autocast('cuda') if self.scaler else nullcontext()

NEW MEDIUM #44: CheckpointManager expects config but receives dataclass
STATUS: ⚠️ STILL PRESENT
Evidence: checkpoint_manager.py line 48 uses vars() which doesn't work for dataclass
          Should use asdict() from dataclasses

NEW MEDIUM #45: train.py early stopping uses val_exact_match but evaluator broken
STATUS: ✅ FIXED (evaluator now works correctly)

NEW MEDIUM #46: Trainer warmup_steps may exceed total_steps
STATUS: ✅ FIXED
Evidence: train.py lines 399-403 validates warmup_steps

NEW MEDIUM #47: scan_dataset.py test file section uses deprecated API
STATUS: ✅ FIXED
Evidence: scan_dataset.py lines 297-315 now uses SCANDataCollator

NEW LOW #48-50: Code quality issues
STATUS: Mixed


================================================================================
                      PART 2: NEW BUGS DISCOVERED
                          (Third Review Pass)
================================================================================

------------------------------------------------------------------------------
                       NEW CRITICAL BUGS
------------------------------------------------------------------------------

NEW CRITICAL #51: SCITrainer doesn't have OverfittingDetector
--------------------------------------------------------------------------------
File: sci/training/trainer.py
Issue: Unlike train.py which creates and uses OverfittingDetector, SCITrainer
       class doesn't implement overfitting detection at all.
Impact: When using SCITrainer class (not train.py), overfitting won't be detected.
Fix: Add OverfittingDetector to SCITrainer.__init__ and use in train_epoch()


NEW CRITICAL #52: SCITrainer.train() uses wrong SCIEvaluator API
--------------------------------------------------------------------------------
File: sci/training/trainer.py lines 483-489
Code:
    if evaluator is not None:
        eval_results = evaluator.evaluate(model=self.model)
        first_dataset = list(eval_results.keys())[0] if eval_results else None

Issue: SCITrainer calls evaluator.evaluate(model=self.model) but SCANEvaluator
       expects evaluate(model, test_dataloader, device).
       
       sci/evaluation/scan_evaluator.py line 52:
       def evaluate(self, model, test_dataloader, device='cuda')
       
       But SCITrainer calls with just model as keyword arg.
       
Impact: Evaluation in SCITrainer.train() will fail with TypeError.
Fix: SCITrainer needs to create dataloader and pass to evaluator


------------------------------------------------------------------------------
                        NEW HIGH BUGS
------------------------------------------------------------------------------

NEW HIGH #53: SCIEvaluator vs SCIEvaluator name inconsistency
--------------------------------------------------------------------------------
Files: 
  - sci/evaluation/evaluator.py contains SCIEvaluator class
  - sci/evaluation/scan_evaluator.py contains SCANEvaluator class
  - train.py imports SCANEvaluator
  - trainer.py references SCIEvaluator in docstring but doesn't import

Issue: Two different evaluator classes exist with similar names. The class-based
       trainer (trainer.py) docstrings reference SCIEvaluator but actual usage
       requires SCANEvaluator.
       
Impact: Confusion about which evaluator to use.


NEW HIGH #54: train.py uses config.loss.ortho_weight but should be ortho_weight
--------------------------------------------------------------------------------
File: train.py line 406
Code: criterion = SCICombinedLoss(
    scl_weight=config.loss.scl_weight,
    ortho_weight=config.loss.ortho_weight,  # <-- This is correct
    temperature=config.loss.scl_temperature,
)

File: config_loader.py line 197
Code: ortho_weight: float = 0.1

Status: ACTUALLY OK - ortho_weight exists in LossConfig. No bug.


NEW HIGH #55: trainer.py model.from_pretrained uses wrong API
--------------------------------------------------------------------------------
File: sci/training/trainer.py line 560
Code: self.model = self.model.from_pretrained(model_path, config=self.config)

Issue: from_pretrained is being called on the model instance, but should be
       called on the class. Also, SCI model may not support from_pretrained
       the same way as HuggingFace models.
       
Impact: Checkpoint loading may fail.


NEW HIGH #56: SCITrainer missing validation dataset
--------------------------------------------------------------------------------
File: sci/training/trainer.py

Issue: SCITrainer only creates train_dataset and train_loader. There is no
       validation dataset or validation loop. The evaluator is optional and
       only called if passed to train().
       
Impact: Training has no validation loss tracking - only training loss.


------------------------------------------------------------------------------
                       NEW MEDIUM BUGS
------------------------------------------------------------------------------

NEW MEDIUM #57: train.py warmup_steps validation uses wrong variable
--------------------------------------------------------------------------------
File: train.py lines 397-403
Code:
    if getattr(config.training.optimizer, 'use_scheduler', False):
        total_steps = total_epochs * len(train_loader)
        warmup_steps = getattr(config.training, 'warmup_steps', 1000)
        # #46: Validate warmup_steps < total_steps
        if warmup_steps >= total_steps:

Issue: warmup_steps validation happens inside the scheduler block, but
       config.training.warmup_steps is used elsewhere (e.g., trainer.py line 99)
       without validation.
       
Impact: warmup_steps might exceed total_steps if scheduler is not used.


NEW MEDIUM #58: scan_evaluator.py generates beyond max_length without warning
--------------------------------------------------------------------------------
File: sci/evaluation/scan_evaluator.py lines 103-106
Code:
    max_total_length = min(inst_length + max_output_tokens, 2048)
    outputs = model.generate(
        max_length=max_total_length,
        ...
    )

Issue: max_total_length uses hardcoded 2048 limit. For very long sequences,
       this could still exceed model's actual max position embeddings (1024
       for some models).
       
Impact: RuntimeError for very long generations.


NEW MEDIUM #59: trainer.py uses deprecated torch.cuda.amp.autocast
--------------------------------------------------------------------------------
File: sci/training/trainer.py line 369
Code: amp_context = torch.amp.autocast('cuda') if self.scaler else nullcontext()

Status: ACTUALLY OK - This is the new API (torch.amp.autocast). No bug.


NEW MEDIUM #60: checkpoint_manager.py config serialization uses vars()
--------------------------------------------------------------------------------
File: sci/training/checkpoint_manager.py
Evidence: Uses vars(config) for dataclass which may not work properly

Issue: vars() on dataclass doesn't recursively convert nested dataclasses.
       Should use dataclasses.asdict() instead.
       
Impact: Checkpoint config may not serialize properly.


NEW MEDIUM #61: Multiple .py files have Windows-incompatible multiprocessing
--------------------------------------------------------------------------------
Files: train.py, trainer.py, scan_evaluator.py
Code: num_workers=0  # Windows compatibility

Issue: All files set num_workers=0 which is correct for Windows but suboptimal
       for Linux. Should use config setting or platform detection.
       
Impact: Suboptimal data loading on Linux systems.


------------------------------------------------------------------------------
                        NEW LOW BUGS
------------------------------------------------------------------------------

NEW LOW #62: Duplicate imports in some files
--------------------------------------------------------------------------------
Files: train.py, trainer.py
Issue: Some imports are duplicated or unused.


NEW LOW #63: Inconsistent error handling patterns
--------------------------------------------------------------------------------
Files: Multiple
Issue: Some functions raise exceptions, others return None, others print warnings.
       No consistent error handling strategy.


NEW LOW #64: Missing __all__ exports in __init__.py files
--------------------------------------------------------------------------------
Files: sci/**/__init__.py
Issue: Most __init__.py files are empty or don't export symbols properly.


NEW LOW #65: Test files in main codebase reference deprecated APIs
--------------------------------------------------------------------------------
File: sci/data/datasets/scan_dataset.py __main__ section
Issue: Test section is updated but still tests deprecated patterns.


================================================================================
                              SUMMARY
================================================================================

PREVIOUS BUGS (50):
| Status      | Count |
|-------------|-------|
| ✅ FIXED    |  33   |
| ⚠️ PRESENT  |  12   |
| Low Priority|   5   |

NEW BUGS FOUND (15):
| Severity | Count |
|----------|-------|
| CRITICAL |   2   |
| HIGH     |   4   |
| MEDIUM   |   5   |
| LOW      |   4   |

TOTAL OPEN BUGS: 17 (including new bugs, excluding low priority)

================================================================================
                         PRIORITY FIX ORDER
================================================================================

IMMEDIATE PRIORITY (Block SCITrainer usage):
1. NEW CRITICAL #51: SCITrainer missing OverfittingDetector
2. NEW CRITICAL #52: SCITrainer.train() uses wrong evaluator API

HIGH PRIORITY:
3. NEW HIGH #55: trainer.py model.from_pretrained wrong API
4. NEW HIGH #56: SCITrainer missing validation dataset
5. NEW MEDIUM #60: checkpoint_manager.py config serialization

MEDIUM PRIORITY (Code quality):
6. HIGH #39: OverfittingDetector not used in SCITrainer
7. NEW MEDIUM #58: scan_evaluator.py hardcoded 2048 limit
8. MEDIUM #44: CheckpointManager vars() issue
9. MEDIUM #25: Duplicate training scripts

LOW PRIORITY (Non-blocking):
- Remaining code quality issues
- Unused config fields
- Platform-specific optimizations

================================================================================
                         RECOMMENDATIONS
================================================================================

1. Focus on fixing SCITrainer class to match train.py functionality
2. Consolidate to single training entry point (either train.py or SCITrainer)
3. Add integration tests that verify train.py and SCITrainer produce same results
4. Consider deprecating one of the training approaches

================================================================================
                           END OF REPORT
================================================================================
