# Baseline Configuration
# Standard TinyLlama fine-tuning WITHOUT SCI components
# Expected to fail on OOD generalization (<20% on length split)

seed: 42

# Model Architecture
model:
  # Base TinyLlama model (same as SCI)
  base_model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  d_model: 2048
  num_decoder_layers: 22

  # Positional Encoding
  position_encoding:
    type: "rotary"
    max_length: 512
    base: 10000

  # ALL SCI COMPONENTS DISABLED
  structural_encoder:
    enabled: false

  content_encoder:
    enabled: false

  causal_binding:
    enabled: false

# Data Configuration (separate from training for schema compliance)
data:
  dataset: "scan"
  split: "length"
  max_length: 512  # SCAN length split can have outputs up to 288 tokens
  scl_ratio: 0.0   # No SCL for baseline
  num_workers: 4
  pairs_cache_dir: ".cache/scan"
  use_chat_template: true  # Same as SCI for fair comparison

# Training Configuration (same as SCI for fair comparison)
training:
  # Batch configuration (same as SCI)
  batch_size: 32
  gradient_accumulation_steps: 1

  # Training duration
  max_epochs: 50
  gradient_clip: 1.0
  warmup_steps: 1000

  # Mixed precision (use mixed_precision, not fp16)
  mixed_precision: true

  # Evaluation frequency
  eval_freq: 1
  save_every: 5

  # Optimizer (same as SCI for fair comparison)
  optimizer:
    type: "AdamW"
    base_lr: 2.0e-5
    sci_lr: 2.0e-5     # Same as base for baseline (no SCI modules)
    lr: 2.0e-5
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-8

  # Scheduler
  scheduler:
    type: "cosine"
    num_training_steps: 50000
    num_warmup_steps: 1000

# Checkpointing Configuration
checkpointing:
  save_dir: "checkpoints/baseline"
  keep_last_n: 3

# Loss Configuration (only LM loss)
loss:
  task_weight: 1.0
  
  # NO SCI losses (set to 0 for baseline)
  scl_weight: 0.0
  ortho_weight: 0.0
  eos_weight: 2.0  # Still use EOS weighting for proper termination
  
  # These must be set even if unused (for config validation)
  scl_temperature: 0.07
  scl_warmup_steps: 0

# Evaluation Configuration
evaluation:
  batch_size: 64

  datasets:
    - name: "scan"
      split: "length"
      subset: "test"
    - name: "scan"
      split: "simple"
      subset: "test"

  max_generation_length: 300   # FIXED: Increased from 128 to 300 (SCAN length needs >=288)
  num_beams: 1                 # Greedy decoding
  do_sample: false
  repetition_penalty: 1.0      # ADDED: Explicit (no penalty)
  length_penalty: 1.0          # ADDED: Explicit (no penalty)

  compute_exact_match: true
  compute_structural_invariance: false  # N/A for baseline

# Logging
logging:
  use_wandb: true
  wandb_project: "SCI-Nature"
  wandb_tags: ["baseline", "scan_length", "tinyllama"]
  log_every: 10

  log_dir: "logs/baseline"
  results_dir: "results/baseline"

# Expected Results
expected_results:
  scan_length_id: 0.95      # Should be similar to SCI
  scan_length_ood: 0.20     # POOR OOD performance (this is the key difference)
  scan_simple: 0.95         # Should be similar to SCI
  structural_invariance: 0.42  # Low (no structural learning)
