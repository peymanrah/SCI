# Base Configuration for SCI
# This file contains all hyperparameters for the SCI architecture

seed: 42

# Model Architecture
model:
  # Base model (TinyLlama-1.1B)
  base_model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  d_model: 2048  # TinyLlama hidden size
  num_decoder_layers: 22  # TinyLlama num layers

  # SCI feature flags
  use_abstraction_layer: true
  use_scl: true
  use_orthogonality_loss: true
  use_causal_binding: true

  # Projection dimension (SCI encoders â†’ TinyLlama)
  projection_dim: 2048

  # Positional Encoding
  # NeuroGen Transfer: 2048 max_length for safe extrapolation margin
  position_encoding:
    type: "rotary"  # or "alibi"
    max_length: 2048  # Increased from 512 (NeuroGen: larger margin for length split)
    base: 10000  # RoPE base

  # Structural Encoder
  structural_encoder:
    num_slots: 8  # 8 slots for SCAN complexity (max ~9 compositional elements)
    num_layers: 12  # Deep for hierarchical abstraction
    d_model: 512  # Balanced capacity/efficiency
    num_heads: 8  # 64-dim heads (512/8)
    dim_feedforward: 2048  # 4x d_model
    dropout: 0.15  # Increased from 0.1 (NeuroGen: better OOD regularization)

    # AbstractionLayer - THE KEY INNOVATION
    # Learns token-wise structuralness scores in [0,1]
    # Higher dropout for novel module (NeuroGen: TIB-like component)
    abstraction_layer:
      hidden_multiplier: 2  # MLP hidden = d_model * 2 = 1024
      residual_init: 0.1    # Small init encourages strong abstraction
      temperature: 0.1
      dropout: 0.2          # Higher dropout for novel module (NeuroGen pattern)
      injection_layers: [3, 6, 9]  # Multi-scale: 25%, 50%, 75% depth

    # Slot Attention pools variable-length to fixed slots
    # NeuroGen Transfer: 6 iterations for complex compositional structures
    slot_attention:
      num_iterations: 6  # Increased from 3 (NeuroGen empirical finding)
      epsilon: 1e-8      # Numerical stability

  # Content Encoder
  # NOTE: Lightweight (2 layers) since content is well-represented in pretrained embeddings
  content_encoder:
    num_layers: 2  # Lightweight - pretrained embeddings already capture content
    d_model: 512
    num_heads: 8
    dim_feedforward: 2048
    dropout: 0.1
    pooling: "mean"

  # Causal Binding Mechanism
  causal_binding:
    # For TinyLlama (22 layers): inject at ~27%, ~50%, ~73% depth
    injection_layers: [6, 11, 16]  # Inject into TinyLlama at early/mid/late layers
    num_heads: 8
    dropout: 0.15  # NeuroGen: binding must generalize
    use_causal_intervention: true

# Training
# NeuroGen Transfer: Longer training, tighter gradient clip
training:
  batch_size: 32
  gradient_accumulation_steps: 2  # Effective batch = 64
  max_epochs: 100  # NeuroGen: Full convergence
  gradient_clip: 0.5  # NeuroGen: Tighter clipping
  warmup_steps: 1000
  mixed_precision: true  # Use fp16
  eval_freq: 1
  save_every: 10

  # Optimizer
  optimizer:
    type: "AdamW"
    base_lr: 2.0e-5  # Base model learning rate
    sci_lr: 6.0e-5   # SCI modules learning rate (3x higher)
    lr: 2.0e-5  # Default/fallback LR
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8

  # Scheduler
  scheduler:
    type: "cosine"
    num_training_steps: 50000
    num_warmup_steps: 1000

# Loss Weights
# NeuroGen Transfer: Sharper temperature, stronger SCL weight, more EOS emphasis
loss:
  task_weight: 1.0  # Standard seq2seq loss
  scl_weight: 0.5  # Increased from 0.3 (NeuroGen: stronger structural learning)
  scl_warmup_steps: 5000  # Gradually increase SCL weight
  scl_warmup_epochs: 3  # Number of epochs for SCL warmup (NeuroGen: longer warmup)
  ortho_weight: 0.1  # Orthogonality constraint
  eos_weight: 3.0  # Increased from 2.0 (NeuroGen: reliable sequence termination)
  scl_temperature: 0.05  # Decreased from 0.07 (NeuroGen: sharper discrimination)

# Data
data:
  dataset: "scan"
  split: "simple"  # Training split
  max_length: 512
  scl_ratio: 0.5  # 50% of batch for SCL pairs
  num_workers: 4

# Evaluation
evaluation:
  batch_size: 64
  beam_size: 1  # Greedy decoding for fair comparison
  max_generation_length: 512

# Logging
logging:
  wandb_project: "SCI-SCAN"
  wandb_entity: null
  wandb_tags: []
  log_every_n_steps: 100
  checkpoint_every_n_epochs: 5

# Checkpointing
checkpointing:
  save_dir: "checkpoints"
  keep_last_n: 3
