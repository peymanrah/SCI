# Base Configuration for SCI
# This file contains all hyperparameters for the SCI architecture

seed: 42

# Model Architecture
model:
  # Base model (TinyLlama-1.1B)
  base_model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  d_model: 2048  # TinyLlama hidden size
  num_decoder_layers: 22  # TinyLlama num layers

  # SCI feature flags
  use_abstraction_layer: true
  use_scl: true
  use_orthogonality_loss: true
  use_causal_binding: true

  # Projection dimension (SCI encoders â†’ TinyLlama)
  projection_dim: 2048

  # Positional Encoding
  position_encoding:
    type: "rotary"  # or "alibi"
    max_length: 512
    base: 10000  # RoPE base

  # Structural Encoder
  structural_encoder:
    num_slots: 8  # 8 slots for SCAN complexity (max ~9 compositional elements)
    num_layers: 12  # Deep for hierarchical abstraction
    d_model: 512  # Balanced capacity/efficiency
    num_heads: 8  # 64-dim heads (512/8)
    dim_feedforward: 2048  # 4x d_model
    dropout: 0.1

    # AbstractionLayer - THE KEY INNOVATION
    # Learns token-wise structuralness scores in [0,1]
    abstraction_layer:
      hidden_multiplier: 2  # MLP hidden = d_model * 2 = 1024
      residual_init: 0.1    # Small init encourages strong abstraction
      temperature: 0.1
      dropout: 0.1
      injection_layers: [3, 6, 9]  # Multi-scale: 25%, 50%, 75% depth

    # Slot Attention pools variable-length to fixed slots
    slot_attention:
      num_iterations: 3  # Iterative slot refinement
      epsilon: 1e-8      # Numerical stability

  # Content Encoder
  # NOTE: Lightweight (2 layers) since content is well-represented in pretrained embeddings
  content_encoder:
    num_layers: 2  # Lightweight - pretrained embeddings already capture content
    d_model: 512
    num_heads: 8
    dim_feedforward: 2048
    dropout: 0.1
    pooling: "mean"

  # Causal Binding Mechanism
  causal_binding:
    # For TinyLlama (22 layers): inject at ~27%, ~50%, ~73% depth
    injection_layers: [6, 11, 16]  # Inject into TinyLlama at early/mid/late layers
    num_heads: 8
    dropout: 0.1
    use_causal_intervention: true

# Training
training:
  batch_size: 32
  max_epochs: 50
  gradient_clip: 1.0
  warmup_steps: 1000
  mixed_precision: true  # Use fp16

  # Optimizer
  optimizer:
    type: "AdamW"
    lr: 2e-5  # Base learning rate
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8

  # Scheduler
  scheduler:
    type: "cosine"
    num_training_steps: 50000
    num_warmup_steps: 1000

# Loss Weights
loss:
  task_weight: 1.0  # Standard seq2seq loss
  scl_weight: 0.3  # Structural contrastive learning
  scl_warmup_steps: 5000  # Gradually increase SCL weight
  ortho_weight: 0.1  # Orthogonality constraint
  eos_weight: 2.0  # Upweight EOS token
  scl_temperature: 0.07  # Temperature for contrastive loss

# Data
data:
  dataset: "scan"
  split: "simple"  # Training split
  max_length: 512
  scl_ratio: 0.5  # 50% of batch for SCL pairs
  num_workers: 4

# Evaluation
evaluation:
  batch_size: 64
  beam_size: 1  # Greedy decoding for fair comparison
  max_generation_length: 512

# Logging
logging:
  wandb_project: "SCI-SCAN"
  wandb_entity: null
  wandb_tags: []
  log_every_n_steps: 100
  checkpoint_every_n_epochs: 5

# Checkpointing
checkpointing:
  save_dir: "checkpoints"
  keep_last_n: 3
