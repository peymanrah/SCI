# SCI Full Configuration
# Complete SCI architecture with all components enabled
# Target: >85% OOD accuracy on SCAN length split

# Inherits from base_config.yaml and overrides specific values

seed: 42

# Model Architecture
model:
  # Base TinyLlama model
  base_model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  d_model: 2048  # TinyLlama hidden dimension
  num_decoder_layers: 22

  # Positional Encoding (RoPE for length generalization)
  position_encoding:
    type: "rotary"
    max_length: 512
    base: 10000

  # Structural Encoder - ENABLED
  structural_encoder:
    enabled: true
    num_slots: 8  # Number of abstract structural slots
    num_layers: 12  # Transformer encoder layers
    d_model: 512   # SE dimension
    num_heads: 8
    dim_feedforward: 2048
    dropout: 0.1

    # AbstractionLayer - THE KEY INNOVATION
    abstraction_layer:
      hidden_multiplier: 2  # MLP hidden size = d_model * multiplier
      residual_init: 0.1    # Initial residual gate value
      dropout: 0.1
      injection_layers: [3, 6, 9]  # Inject AbstractionLayer at these SE layers

    # Slot Attention pooling
    slot_attention:
      num_iterations: 3
      epsilon: 1e-8

  # Content Encoder - ENABLED
  content_encoder:
    enabled: true
    num_layers: 2    # Lightweight (refines shared embeddings)
    d_model: 512
    num_heads: 8
    dim_feedforward: 2048
    dropout: 0.1
    pooling: "mean"  # Pool to [batch, d_model]
    use_orthogonal_projection: false  # Use loss-based orthogonality instead

  # Causal Binding Mechanism - ENABLED
  causal_binding:
    enabled: true
    d_model: 2048    # TinyLlama dimension
    num_heads: 8
    dropout: 0.1
    injection_layers: [6, 11, 16]  # Inject at early, middle, late TinyLlama layers
    use_causal_intervention: true

# Data Configuration
data:
  dataset: "scan"
  split: "length"    # Train on length split (OOD generalization test)
  max_length: 128    # Sufficient for SCAN
  scl_ratio: 0.5
  num_workers: 4

# Training Configuration
training:
  batch_size: 32
  max_epochs: 50
  gradient_clip: 1.0
  warmup_steps: 1000
  mixed_precision: true  # Use fp16
  eval_freq: 1           # BUG #31 FIX: Evaluate every N epochs (1 = every epoch)
  save_every: 5          # Save checkpoint every N epochs

  optimizer:
    type: "AdamW"
    base_lr: 2.0e-5      # Base model learning rate
    sci_lr: 5.0e-5       # SCI modules learning rate (2.5x higher)
    lr: 2.0e-5           # Default LR (for backwards compatibility)
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8

  scheduler:
    type: "cosine"
    num_training_steps: 50000
    num_warmup_steps: 1000

# Loss Configuration
loss:
  task_weight: 1.0
  scl_weight: 0.3
  scl_warmup_steps: 5000
  ortho_weight: 0.1
  eos_weight: 2.0
  scl_temperature: 0.07

# Evaluation Configuration
evaluation:
  batch_size: 64
  beam_size: 1  # Greedy decoding for fair comparison
  max_generation_length: 300  # FIXED: Must be >=288 for SCAN length split
  num_beams: 1
  do_sample: false
  repetition_penalty: 1.0  # ADDED: Explicit (no penalty)
  length_penalty: 1.0      # ADDED: Explicit (no penalty)

# Logging Configuration
logging:
  wandb_project: "SCI-SCAN"
  wandb_entity: null
  wandb_tags: ["sci_full", "scan_length", "tinyllama"]
  log_every_n_steps: 100
  checkpoint_every_n_epochs: 5

# Checkpointing Configuration
checkpointing:
  save_dir: "checkpoints"
  keep_last_n: 3

# Expected Results
# These are target metrics for verification
expected_results:
  scan_length_id: 0.95   # In-distribution
  scan_length_ood: 0.85  # Out-of-distribution (length generalization)
  scan_simple: 0.98      # Standard split
  structural_invariance: 0.89  # Same-structure similarity
